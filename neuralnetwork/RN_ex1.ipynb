{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnLFFG6e5sug"
      },
      "source": [
        "# Prática I - Redes Neurais usando Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn   #Definicao da rede neural\n",
        "from torch.utils.data import Dataset, DataLoader  #utilidade para dataset\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset e pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste exemplo, iremos usar o dataset Iris. É um problema bastante conhecido na área de Ciência de Dados.\n",
        "A base de dados possui 150 registros e 4 atributos, com informações de comprimento e largura de suas pétalas e sépalas. No total, temos 50 amostras para cada uma das três classes: Iris-Setosa, Iris-Verginica e Iris-Versicolor\n",
        "\n",
        "<img src=\"https://www.embedded-robotics.com/wp-content/uploads/2022/01/Iris-Dataset-Classification.png\"/>\n",
        "\n",
        "Image source: https://www.embedded-robotics.com/iris-dataset-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_iris()\n",
        "X = data.data\n",
        "target = data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vejamos como são as entradas e saídas do modelo. O que precisa ser feito inicialmente?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precisamos também dividir a base de dados entre treinamento, validação e teste.\n",
        "\n",
        "Usaremos a função `train_test_split` do scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E normalizar os dados. Podemos fazer esse processo manualmente ou usando alguma função já implementada (e.g. `MinMaxScaler` do scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Criando o dataset e definindo o DataLoader para o treinamento do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para o treinamento de um modelo de Redes Neurais, precisamos definir uma estrutura de dataset para organização dos dados, amostragem de dados e outras técnicas possíveis, como data augmentation.\n",
        "Também precisamos definir um DataLoader. \n",
        "\n",
        "Lembrete: Dependendo do tipo de problema, devemos adequar o tipo das variáveis. No exemplo abaixo, estamos adequando as variáveis X_train, X_val e X_test do tipo double para float. Em alguns casos (especialmente em problemas com múltiplas classes), temos que adequar as saídas desejadas para o tipo long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Função necessária para criar um custom dataset no Pytorch\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Criando um modelo de Redes Neurais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos criar um objeto que irá representar o nosso modelo de Redes Neurais, que irá herdar características de `torch.nn.Module`. No `__init__` iremos definir sua arquitetura e quaisquer outras características que sejam importantes.\n",
        "\n",
        "No forward iremos definir como ocorrerá a propagação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Criar usando classe. \n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "    \n",
        "  def forward(self,x):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste caso, definimos um loop para o treinamento por *n_epochs* épocas. Neste caso, não estamos usando o \"offline learning\", ensinado em sala de aula, mas sim um aprendizado em lote (batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos agora avaliar o modelo treinado. Para isso, mudamos o modelo para o modo eval. Além disso, usamos o `torch.no_grad()`, para que a execução do trecho de código tenha o autograd engine desativado. \n",
        "\n",
        "Lembrete: Caso queira continuar o treinamento do modelo, é necessário usar o comando `model.train()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  X_test_torch = torch.from_numpy(X_test).float()\n",
        "  y_hat = model(X_test_torch)\n",
        "  y_ = torch.argmax(y_hat.data,dim = 1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métricas de avaliação para classificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=False):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(cm, data.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Material adicional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un2nHgZG5suj"
      },
      "source": [
        "## Parte 1 - Construindo uma rede do zero\n",
        "\n",
        "Redes Neurais constituem uma das técnicas mais populares de Inteligência Computacional. Por mais que esta técnica tenha um potencial para mapeamento de diversos problemas, o conceito básico sobre o seu funcionamento não é impossível de ser entendido.\n",
        "\n",
        "Nesta primeira parte prática, iremos construir uma Rede Neural do zero. Para uma forma didática, a primeira etapa vai abordar os principais aspectos de Redes Neurais. \n",
        "\n",
        "Resolva primeiro a parte teórica do item para resolver em seguida a parte prática.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFEJ7kRv5suk"
      },
      "source": [
        "O esqueleto do modelo de Rede Neural já está pronto. Para uma melhor comodidade, o script já possui os métodos essenciais para o funcionamento, mas os códigos ainda não estão implementados. O objetivo deste trabalho é desenvolver o conhecimento teórico e prático sobre redes neurais, entendendo mais a fundo as suas operações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAxlSona5sul"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Zm0gi05sum"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self,nin,nout,hidden_neurons=[2],hidden_layers=1,activation=['relu']):\n",
        "        '''\n",
        "        Classe para a implementação do algoritmo de Redes Neurais para a atividade prática da disciplina Inteligência Computacional Aplicada - PUC-Rio\n",
        "        \n",
        "        Variáveis de entrada:\n",
        "        nin - Quantidade de entradas da Rede (int)\n",
        "        nout - Quantidade de neurônios na saída da Rede (int)\n",
        "        hidden_neurons - Quantidade de neurônios na camada escondida. Deve ser uma lista de inteiros positivos.\n",
        "        hidden_layers - Quantidade de camadas escondidas. (int)\n",
        "        activation - Função de ativação para cada uma das camadas. Deve ser uma lista de strings.\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        #Entrada do modelo\n",
        "        self.input = nin \n",
        "        #Saída do modelo\n",
        "        self.output = nout \n",
        "        #Lista de neurônios. Começa a contagem da entrada do modelo.\n",
        "        self.hn = [nin] + hidden_neurons \n",
        "        \n",
        "        self.hl = hidden_layers\n",
        "\n",
        "        #Dicionário com todas as possíveis funções de ativação. Caso construa alguma outra, adicione a esta lista.\n",
        "        self.possible_activations = {'relu':self.relu, 'tanh':np.tanh, 'sigmoid':self.sigmoid}\n",
        "\n",
        "        #Inicialização dos pesos ao criar o novo objeto\n",
        "        self.initialize_weights(activation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def initialize_weights(self,activation):\n",
        "\n",
        "        #Matriz de pesos \n",
        "        self.weights = dict()\n",
        "        #Vetor de bias\n",
        "        self.bias = dict()\n",
        "\n",
        "        #Erros\n",
        "        self.weights_error = dict()\n",
        "        self.bias_error = dict()\n",
        "\n",
        "        #Ativacao do neuronio\n",
        "        self.neuron_out = dict()\n",
        "        \n",
        "        \n",
        "        self.neuron_net = dict()\n",
        "\n",
        "        self.activation = dict()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(self.hl):\n",
        "            name = f'hidden{i}'\n",
        "\n",
        "            self.weights[name] = np.random.rand(self.hn[i+1], self.hn[i])*2 - 1.0\n",
        "            self.bias[name] = np.zeros((self.hn[i+1],1))\n",
        "\n",
        "            self.weights_error[name] = np.zeros((self.hn[i+1], self.hn[i]))\n",
        "            self.bias_error[name] = np.zeros((self.hn[i+1],1))\n",
        "\n",
        "            self.activation[name] = self.possible_activations[activation[i]]\n",
        "\n",
        "            self.neuron_out[name] = np.zeros((self.hn[i+1],1))\n",
        "            self.neuron_net[name] = np.zeros((self.hn[i], 1))\n",
        "\n",
        "\n",
        "        self.weights['output'] = np.random.rand(self.output, self.hn[-1])*2 - 1.0\n",
        "        self.bias['output'] = np.zeros((self.output, 1))\n",
        "\n",
        "        self.weights_error['output'] = np.zeros((self.output, self.hn[-1]))\n",
        "        self.bias_error['output'] = np.zeros((self.output,1))\n",
        "\n",
        "        self.activation['output'] = self.possible_activations[activation[-1]]\n",
        "\n",
        "        self.neuron_out['output'] = np.zeros((self.output,1))\n",
        "\n",
        "        self.neuron_net['output'] = np.zeros((self.hn[-1],1))\n",
        "\n",
        "    def insert_layers(self,weights=[],bias=[]):\n",
        "        '''\n",
        "        Função para inserção manual dos pesos. \n",
        "        '''\n",
        "        self.weights = weights\n",
        "        self.bias = bias \n",
        "\n",
        "    def neuron_process(self,x=[],weights=[],bias=[]):\n",
        "        '''\n",
        "        Função para regra de propagação\n",
        "        #NOTE\n",
        "        Etapa 1\n",
        "        '''\n",
        "        return None\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        '''\n",
        "        Função para etapa feedforward\n",
        "        #NOTE\n",
        "        Etapa 3\n",
        "        '''\n",
        "        input_values = x\n",
        "        for layer in self.weights:\n",
        "            pass\n",
        "            \n",
        "        return None\n",
        "\n",
        "\n",
        "    def backward(self,lr,y):\n",
        "        '''\n",
        "        Função para a etapa feedback\n",
        "        '''\n",
        "        for layer in reversed(self.weights):\n",
        "\n",
        "            if layer == 'output':\n",
        "                dZ2 = y - self.neuron_out[layer]\n",
        "                dW2 = np.dot(dZ2, self.neuron_net[layer].T)\n",
        "                db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
        "                self.weights_error[layer] = lr * dW2 \n",
        "                self.bias_error[layer] = lr * db2 \n",
        "                prev_weights = self.weights[layer]\n",
        "            else:\n",
        "                dZ1 = np.multiply(np.dot(prev_weights.T, dZ2), 1-np.power(self.neuron_out[layer], 2))\n",
        "                dW1 = np.dot(dZ1, self.neuron_net[layer].T)\n",
        "                db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
        "                self.weights_error[layer] = lr * dW1 \n",
        "                self.bias_error[layer] = lr * db1 \n",
        "\n",
        "                prev_weights = self.weights[layer]\n",
        "                dZ2 = np.copy(dZ1)\n",
        "            \n",
        "\n",
        "    def update_weights(self):\n",
        "        '''\n",
        "        Função para atualização dos pesos\n",
        "        #NOTE\n",
        "        Etapa 5\n",
        "        '''\n",
        "        for layer in self.weights:\n",
        "\n",
        "            assert(self.weights[layer].shape == self.weights_error[layer].shape), f\"Dimension mismatch: {self.weights[layer].shape} and {self.weights_error[layer].shape}\"\n",
        "            self.weights[layer] = self.weights[layer] + self.weights_error[layer]\n",
        "\n",
        "            assert(self.bias[layer].shape == self.bias_error[layer].shape), f\"Dimension mismatch: {self.bias[layer].shape} and {self.bias_error[layer].shape}\"\n",
        "            self.bias[layer] = self.bias[layer] + self.bias_error[layer]\n",
        "            self.weights_error[layer] = np.zeros(self.weights_error[layer].shape)\n",
        "            self.bias_error[layer] = np.zeros(self.bias_error[layer].shape)\n",
        "\n",
        "    def train(self,x,y,epochs=100,lr=0.5):\n",
        "        '''\n",
        "        Função para treinamento da rede\n",
        "        # NOTE\n",
        "        Etapa 6\n",
        "        '''\n",
        "        for epoch in range(epochs):\n",
        "            pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        '''\n",
        "        Definição da função sigmoide. Retorna o valor 1/(1 + e^(-x))\n",
        "        #NOTE\n",
        "        Etapa 2\n",
        "        '''\n",
        "        \n",
        "        return None\n",
        "\n",
        "    \n",
        "    def sigmoid_derivative(self,value):\n",
        "        '''\n",
        "        ???\n",
        "        '''\n",
        "        return self.sigmoid(value)*(1-self.sigmoid(value))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(np.zeros(x.shape),x)\n",
        "\n",
        "    @staticmethod \n",
        "    def relu_derivative(x):\n",
        "        return np.ones(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUfLSc545suq"
      },
      "source": [
        "### Questão 1\n",
        "O neurônio é a unidade básica de uma Rede Neural. Na primeira etapa, desenvolva dentro da função **neuron_process** para o cálculo de processamento do neurônio. Não esqueça que a função recebe como argumento a matriz de pesos **weights**, a entrada **x** e o bias **bias**.\n",
        "\n",
        "Ao terminar de implementar a função, valide seu código com os testes abaixo. **Não** prossiga para o próximo item enquanto houver erro no resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX3FhUYJ5suq"
      },
      "outputs": [],
      "source": [
        "nn_test = NeuralNetwork(2,1,hidden_layers=1,hidden_neurons=[4],activation=['tanh','sigmoid'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVaE4OlV5sur",
        "outputId": "61e9cf40-bbd1-4b0b-98c0-bcb6d78e46f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.43813902],\n",
              "       [1.67767442]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[0],[0]])\n",
        "weights = np.array([[ 3.14708825, -3.02515284],[-3.48236268,  3.60001384]])\n",
        "bias = np.array([[1.43813902],[1.67767442]])\n",
        "\n",
        "nn_test.neuron_process(x=x,weights=weights,bias=bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIm47W9D5sus"
      },
      "source": [
        "### Questão 2\n",
        "Com a código de processamento do neurônio já criado, é preciso desenvolver um método para a função de ativação não-linear. \n",
        "Desenvolva a equação que representa a ativação sigmoide e sua respectiva derivada. Para validar o funcionamento, além de realizar os testes padrões estipulados abaixo, verifique graficamente se as funções estão de acordo com o esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL2GrXHT5sus"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuq9S2nj5sus"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(-10,10,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ANRlFL5sut",
        "outputId": "fd096c2a-df2b-4039-edbe-5c87169fa2cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'sigm(x)')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(t,nn_test.sigmoid(t),linewidth=2,color='g')\n",
        "plt.title('Função sigmoide')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('sigm(x)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oNsrgxg5sut"
      },
      "source": [
        "### Etapa III - Feedforward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nGNMG6n5sut"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grlTbW5b5suu"
      },
      "source": [
        "### Etapa IV - Retropropagação dos erros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0P65uwz5suu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP7XingR5suu"
      },
      "source": [
        "### Etapa V - Atualização dos pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePmWAvtg5suu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RihDEX6I5suu"
      },
      "source": [
        "### Etapa VI - Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5VVuALN5suu"
      },
      "outputs": [],
      "source": [
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "#X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "#Y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "nn_test.train(X,Y,epochs=1500,lr=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ZXFxAF5suv"
      },
      "source": [
        "### Etapa VII - Generalização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNjND9Ux5suv",
        "outputId": "7573874d-77d4-44ed-f1a8-990f5a493f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00030853]]\n",
            "[[0.99744658]]\n",
            "[[0.99772362]]\n",
            "[[0.0030284]]\n"
          ]
        }
      ],
      "source": [
        "X_test = np.array([[0],[0]])\n",
        "print(nn_test.forward(X_test))\n",
        "\n",
        "X_test = np.array([[0],[1]])\n",
        "print(nn_test.forward(X_test))\n",
        "\n",
        "X_test = np.array([[1],[0]])\n",
        "print(nn_test.forward(X_test))\n",
        "\n",
        "X_test = np.array([[1],[1]])\n",
        "print(nn_test.forward(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_a1fGH85suv",
        "outputId": "51324d15-f096-4712-fbb0-fcf55f4ddf1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hidden0': array([[-0.93541997],\n",
              "        [-0.30522097],\n",
              "        [ 1.12782389],\n",
              "        [-0.32628317]]),\n",
              " 'output': array([[2.37573173]])}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn_test.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awuPHspA5suv"
      },
      "outputs": [],
      "source": [
        "manual_weights = dict()\n",
        "manual_bias = dict()\n",
        "manual_weights['hidden0'] = np.array([[ 3.14708825, -3.02515284],\n",
        "        [-3.48236268,  3.60001384]])\n",
        "manual_weights['output'] = np.array([[-5.07678577, -5.01607987]])\n",
        "\n",
        "manual_bias['hidden0'] = np.array([[1.43813902],\n",
        "        [1.67767442]])\n",
        "manual_bias['output'] = np.array([[4.49745273]])\n",
        "\n",
        "nn_test.insert_layers(weights=manual_weights,bias=manual_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RdQccp75suv",
        "outputId": "c16bdaca-b087-4a9f-a217-bfb6d3af5ac1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hidden0': array([[ 3.14708825, -3.02515284],\n",
              "        [-3.48236268,  3.60001384]]),\n",
              " 'output': array([[-5.07678577, -5.01607987]])}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn_test.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LWm3rcY5suw",
        "outputId": "8e924936-2a50-4637-cebe-e4b55e8f5573"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hidden0': array([[-0.97217686],\n",
              "        [ 0.99980005],\n",
              "        [ 0.97393905],\n",
              "        [ 0.53319081]]),\n",
              " 'output': array([[0.0030284]])}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn_test.neuron_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDjKcTtO5suw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZjqxmJh5suw"
      },
      "source": [
        "## Parte 2 - Framework para Redes Neurais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "c8K8SUhN56bB"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TPr7WcxP56C8"
      },
      "outputs": [],
      "source": [
        "#Função necessária para criar um custom dataset no Pytorch\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b9b6sYXp56Ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "P8yNXGSl559W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TxsmymsR8Jcb"
      },
      "outputs": [],
      "source": [
        "data = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yd_HA1QL8JVW"
      },
      "outputs": [],
      "source": [
        "X = data.data\n",
        "target = data.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "U831MSe__cqG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_Oxzac5g_dBV"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CPZAs0VU8JST"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fox8ajPwR8i-"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BDrVIczu82OC"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_dataset = CustomDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_dataset = CustomDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uuZxUNKl82Fq"
      },
      "outputs": [],
      "source": [
        "#@title Definindo dataloader\n",
        "batch_size = 16 #@param {type:\"integer\"}\n",
        "\n",
        "#Gerando um dataloader\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGcMQpul6bOV"
      },
      "source": [
        "Temos algumas formas para criar um modelo de Redes Neurais em Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3Ft_V55Y556e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sXucmYFY553k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bDrfKZF6550c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sU2rn4Ak55xl"
      },
      "outputs": [],
      "source": [
        "#@title Criando uma Rede Neural usando classe.\n",
        "#Criar usando classe. \n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,hidden_neurons = 4, hidden_activation = 'relu', output_activation='softmax', lr = 0.05, n_input = 1, n_output = 1):\n",
        "    # create model\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    self.activations = {'relu': nn.ReLU(), 'sigmoid':nn.Sigmoid(), 'softmax':nn.Softmax()}\n",
        "\n",
        "    self.fc1 = nn.Linear(n_input, hidden_neurons)\n",
        "    self.ha = self.activations[hidden_activation]\n",
        "    self.fc2 = nn.Linear(hidden_neurons, n_output)\n",
        "    #self.out = self.activations[output_activation]\n",
        "\n",
        "  def forward(self,x):\n",
        "    h = self.fc1(x)\n",
        "    h1 = self.ha(h) \n",
        "    y = self.fc2(h1) \n",
        "    #y = self.out(h2)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "brQJ1YK755uU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "T7otCCF555rU"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork(hidden_neurons=5, n_input=4, n_output=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "0fbqqYdJ55oU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "form",
        "id": "nGjkZReJ55ll"
      },
      "outputs": [],
      "source": [
        "#@title Definindo parâmetros do problema.\n",
        "lr = 10e-4 #@param {type:\"number\"}\n",
        "device = 'cuda' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YUi6Znt555id"
      },
      "outputs": [],
      "source": [
        "#@title Definindo a função de perda e o método de otimização para atualização dos pesos.\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "history = {'acc_train' : [], 'loss_train': [], 'acc_val': [], 'loss_val': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CbMSGHJNC0s2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45wT3N5VC0FJ",
        "outputId": "9d851a1c-8f51-40a8-b387-3ea96606d0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, loss 0.6547568440437317\n",
            "epoch 1, loss 0.651190996170044\n",
            "epoch 2, loss 0.6476939916610718\n",
            "epoch 3, loss 0.6442641615867615\n",
            "epoch 4, loss 0.640899658203125\n",
            "epoch 5, loss 0.6375982165336609\n",
            "epoch 6, loss 0.6344050168991089\n",
            "epoch 7, loss 0.6313188076019287\n",
            "epoch 8, loss 0.6283477544784546\n",
            "epoch 9, loss 0.6254609823226929\n",
            "epoch 10, loss 0.6226517558097839\n",
            "epoch 11, loss 0.6198490858078003\n",
            "epoch 12, loss 0.6170814037322998\n",
            "epoch 13, loss 0.6143965721130371\n",
            "epoch 14, loss 0.6117745041847229\n",
            "epoch 15, loss 0.6092046499252319\n",
            "epoch 16, loss 0.6066827774047852\n",
            "epoch 17, loss 0.6042057275772095\n",
            "epoch 18, loss 0.6017715334892273\n",
            "epoch 19, loss 0.5993779897689819\n",
            "epoch 20, loss 0.5970234870910645\n",
            "epoch 21, loss 0.5947062969207764\n",
            "epoch 22, loss 0.5924239158630371\n",
            "epoch 23, loss 0.5901739597320557\n",
            "epoch 24, loss 0.5879561305046082\n",
            "epoch 25, loss 0.5857672095298767\n",
            "epoch 26, loss 0.5836063623428345\n",
            "epoch 27, loss 0.5814740657806396\n",
            "epoch 28, loss 0.5793695449829102\n",
            "epoch 29, loss 0.5772915482521057\n",
            "epoch 30, loss 0.5752384662628174\n",
            "epoch 31, loss 0.5732089281082153\n",
            "epoch 32, loss 0.5712007880210876\n",
            "epoch 33, loss 0.5692124366760254\n",
            "epoch 34, loss 0.5672417879104614\n",
            "epoch 35, loss 0.5653257966041565\n",
            "epoch 36, loss 0.5634571313858032\n",
            "epoch 37, loss 0.5616165995597839\n",
            "epoch 38, loss 0.5597951412200928\n",
            "epoch 39, loss 0.5579856634140015\n",
            "epoch 40, loss 0.5561825633049011\n",
            "epoch 41, loss 0.5543816089630127\n",
            "epoch 42, loss 0.5525771379470825\n",
            "epoch 43, loss 0.5507673025131226\n",
            "epoch 44, loss 0.548947811126709\n",
            "epoch 45, loss 0.5471107959747314\n",
            "epoch 46, loss 0.5452529788017273\n",
            "epoch 47, loss 0.5433734655380249\n",
            "epoch 48, loss 0.541467547416687\n",
            "epoch 49, loss 0.5395270586013794\n",
            "epoch 50, loss 0.5375493168830872\n",
            "epoch 51, loss 0.5355323553085327\n",
            "epoch 52, loss 0.5334751605987549\n",
            "epoch 53, loss 0.5313725471496582\n",
            "epoch 54, loss 0.5292185544967651\n",
            "epoch 55, loss 0.5270063877105713\n",
            "epoch 56, loss 0.5247370600700378\n",
            "epoch 57, loss 0.5224320292472839\n",
            "epoch 58, loss 0.5200853943824768\n",
            "epoch 59, loss 0.5176976919174194\n",
            "epoch 60, loss 0.5152590870857239\n",
            "epoch 61, loss 0.5128525495529175\n",
            "epoch 62, loss 0.5104697346687317\n",
            "epoch 63, loss 0.5081377625465393\n",
            "epoch 64, loss 0.5057969093322754\n",
            "epoch 65, loss 0.5034464597702026\n",
            "epoch 66, loss 0.5010929703712463\n",
            "epoch 67, loss 0.49874162673950195\n",
            "epoch 68, loss 0.4964248538017273\n",
            "epoch 69, loss 0.4941229522228241\n",
            "epoch 70, loss 0.4918212592601776\n",
            "epoch 71, loss 0.4895201325416565\n",
            "epoch 72, loss 0.4872214198112488\n",
            "epoch 73, loss 0.4849238097667694\n",
            "epoch 74, loss 0.48263639211654663\n",
            "epoch 75, loss 0.4803619086742401\n",
            "epoch 76, loss 0.4780940115451813\n",
            "epoch 77, loss 0.47582709789276123\n",
            "epoch 78, loss 0.47360628843307495\n",
            "epoch 79, loss 0.47143587470054626\n",
            "epoch 80, loss 0.4692515730857849\n",
            "epoch 81, loss 0.46705058217048645\n",
            "epoch 82, loss 0.46487870812416077\n",
            "epoch 83, loss 0.4627282917499542\n",
            "epoch 84, loss 0.4605754613876343\n",
            "epoch 85, loss 0.4584231972694397\n",
            "epoch 86, loss 0.4563004970550537\n",
            "epoch 87, loss 0.4541582465171814\n",
            "epoch 88, loss 0.45199838280677795\n",
            "epoch 89, loss 0.4498591721057892\n",
            "epoch 90, loss 0.44769638776779175\n",
            "epoch 91, loss 0.44551536440849304\n",
            "epoch 92, loss 0.44335630536079407\n",
            "epoch 93, loss 0.4412035644054413\n",
            "epoch 94, loss 0.4391304552555084\n",
            "epoch 95, loss 0.4370560050010681\n",
            "epoch 96, loss 0.43499821424484253\n",
            "epoch 97, loss 0.4329681992530823\n",
            "epoch 98, loss 0.4309278726577759\n",
            "epoch 99, loss 0.42890381813049316\n",
            "epoch 100, loss 0.42691758275032043\n",
            "epoch 101, loss 0.42495906352996826\n",
            "epoch 102, loss 0.42296916246414185\n",
            "epoch 103, loss 0.4209533631801605\n",
            "epoch 104, loss 0.418953537940979\n",
            "epoch 105, loss 0.4169680178165436\n",
            "epoch 106, loss 0.4149489104747772\n",
            "epoch 107, loss 0.4129137396812439\n",
            "epoch 108, loss 0.41089603304862976\n",
            "epoch 109, loss 0.4089213013648987\n",
            "epoch 110, loss 0.4069650173187256\n",
            "epoch 111, loss 0.4049798250198364\n",
            "epoch 112, loss 0.4030064344406128\n",
            "epoch 113, loss 0.40105608105659485\n",
            "epoch 114, loss 0.3991141617298126\n",
            "epoch 115, loss 0.39715588092803955\n",
            "epoch 116, loss 0.39517447352409363\n",
            "epoch 117, loss 0.3931986391544342\n",
            "epoch 118, loss 0.3912217915058136\n",
            "epoch 119, loss 0.38925063610076904\n",
            "epoch 120, loss 0.38729751110076904\n",
            "epoch 121, loss 0.3853158950805664\n",
            "epoch 122, loss 0.3833399713039398\n",
            "epoch 123, loss 0.38139063119888306\n",
            "epoch 124, loss 0.379463791847229\n",
            "epoch 125, loss 0.37757983803749084\n",
            "epoch 126, loss 0.3756892681121826\n",
            "epoch 127, loss 0.3738110065460205\n",
            "epoch 128, loss 0.37194934487342834\n",
            "epoch 129, loss 0.3700934946537018\n",
            "epoch 130, loss 0.36821988224983215\n",
            "epoch 131, loss 0.36632272601127625\n",
            "epoch 132, loss 0.36443379521369934\n",
            "epoch 133, loss 0.3625640869140625\n",
            "epoch 134, loss 0.360706090927124\n",
            "epoch 135, loss 0.35887497663497925\n",
            "epoch 136, loss 0.3570535182952881\n",
            "epoch 137, loss 0.3552200198173523\n",
            "epoch 138, loss 0.3533940613269806\n",
            "epoch 139, loss 0.3515811264514923\n",
            "epoch 140, loss 0.3497719168663025\n",
            "epoch 141, loss 0.3479629456996918\n",
            "epoch 142, loss 0.34613561630249023\n",
            "epoch 143, loss 0.3443048298358917\n",
            "epoch 144, loss 0.34248802065849304\n",
            "epoch 145, loss 0.3406817317008972\n",
            "epoch 146, loss 0.33886584639549255\n",
            "epoch 147, loss 0.337053507566452\n",
            "epoch 148, loss 0.33526021242141724\n",
            "epoch 149, loss 0.33348068594932556\n",
            "epoch 150, loss 0.3317120373249054\n",
            "epoch 151, loss 0.32994335889816284\n",
            "epoch 152, loss 0.32821500301361084\n",
            "epoch 153, loss 0.3265179991722107\n",
            "epoch 154, loss 0.3248216509819031\n",
            "epoch 155, loss 0.32309478521347046\n",
            "epoch 156, loss 0.3213697671890259\n",
            "epoch 157, loss 0.3196593225002289\n",
            "epoch 158, loss 0.3179187774658203\n",
            "epoch 159, loss 0.3161691427230835\n",
            "epoch 160, loss 0.31446540355682373\n",
            "epoch 161, loss 0.3127921223640442\n",
            "epoch 162, loss 0.31108561158180237\n",
            "epoch 163, loss 0.3093996047973633\n",
            "epoch 164, loss 0.3076842129230499\n",
            "epoch 165, loss 0.30601465702056885\n",
            "epoch 166, loss 0.3043730854988098\n",
            "epoch 167, loss 0.3027034401893616\n",
            "epoch 168, loss 0.301065593957901\n",
            "epoch 169, loss 0.29944539070129395\n",
            "epoch 170, loss 0.29779988527297974\n",
            "epoch 171, loss 0.296182781457901\n",
            "epoch 172, loss 0.29458287358283997\n",
            "epoch 173, loss 0.292974591255188\n",
            "epoch 174, loss 0.2913669943809509\n",
            "epoch 175, loss 0.28977635502815247\n",
            "epoch 176, loss 0.28815269470214844\n",
            "epoch 177, loss 0.28656071424484253\n",
            "epoch 178, loss 0.28498855233192444\n",
            "epoch 179, loss 0.28342965245246887\n",
            "epoch 180, loss 0.2818797528743744\n",
            "epoch 181, loss 0.28033629059791565\n",
            "epoch 182, loss 0.27879783511161804\n",
            "epoch 183, loss 0.27726417779922485\n",
            "epoch 184, loss 0.27576038241386414\n",
            "epoch 185, loss 0.2743164896965027\n",
            "epoch 186, loss 0.272866427898407\n",
            "epoch 187, loss 0.27139779925346375\n",
            "epoch 188, loss 0.2699187099933624\n",
            "epoch 189, loss 0.26843613386154175\n",
            "epoch 190, loss 0.2669544816017151\n",
            "epoch 191, loss 0.2654772400856018\n",
            "epoch 192, loss 0.26400652527809143\n",
            "epoch 193, loss 0.2625434696674347\n",
            "epoch 194, loss 0.2610887885093689\n",
            "epoch 195, loss 0.2596425414085388\n",
            "epoch 196, loss 0.25820454955101013\n",
            "epoch 197, loss 0.25677451491355896\n",
            "epoch 198, loss 0.2553521394729614\n",
            "epoch 199, loss 0.253937304019928\n",
            "epoch 200, loss 0.25252968072891235\n",
            "epoch 201, loss 0.2511292099952698\n",
            "epoch 202, loss 0.2497357428073883\n",
            "epoch 203, loss 0.2483493536710739\n",
            "epoch 204, loss 0.24697133898735046\n",
            "epoch 205, loss 0.24560165405273438\n",
            "epoch 206, loss 0.24423928558826447\n",
            "epoch 207, loss 0.2428833246231079\n",
            "epoch 208, loss 0.241533562541008\n",
            "epoch 209, loss 0.24018999934196472\n",
            "epoch 210, loss 0.23885275423526764\n",
            "epoch 211, loss 0.23752208054065704\n",
            "epoch 212, loss 0.23619824647903442\n",
            "epoch 213, loss 0.23488134145736694\n",
            "epoch 214, loss 0.23357167840003967\n",
            "epoch 215, loss 0.2322695404291153\n",
            "epoch 216, loss 0.23097458481788635\n",
            "epoch 217, loss 0.22968684136867523\n",
            "epoch 218, loss 0.22840602695941925\n",
            "epoch 219, loss 0.227151021361351\n",
            "epoch 220, loss 0.22588494420051575\n",
            "epoch 221, loss 0.22461673617362976\n",
            "epoch 222, loss 0.2233731746673584\n",
            "epoch 223, loss 0.2221420556306839\n",
            "epoch 224, loss 0.22091029584407806\n",
            "epoch 225, loss 0.21968114376068115\n",
            "epoch 226, loss 0.21845626831054688\n",
            "epoch 227, loss 0.21723659336566925\n",
            "epoch 228, loss 0.2160380333662033\n",
            "epoch 229, loss 0.21484950184822083\n",
            "epoch 230, loss 0.2136581689119339\n",
            "epoch 231, loss 0.21246850490570068\n",
            "epoch 232, loss 0.21128299832344055\n",
            "epoch 233, loss 0.21010304987430573\n",
            "epoch 234, loss 0.20894432067871094\n",
            "epoch 235, loss 0.2077958881855011\n",
            "epoch 236, loss 0.20664533972740173\n",
            "epoch 237, loss 0.20549766719341278\n",
            "epoch 238, loss 0.20435455441474915\n",
            "epoch 239, loss 0.20321671664714813\n",
            "epoch 240, loss 0.2020847201347351\n",
            "epoch 241, loss 0.20097336173057556\n",
            "epoch 242, loss 0.19987185299396515\n",
            "epoch 243, loss 0.19876910746097565\n",
            "epoch 244, loss 0.19770482182502747\n",
            "epoch 245, loss 0.19662120938301086\n",
            "epoch 246, loss 0.19552774727344513\n",
            "epoch 247, loss 0.19443249702453613\n",
            "epoch 248, loss 0.1933581829071045\n",
            "epoch 249, loss 0.19232630729675293\n",
            "epoch 250, loss 0.19127599895000458\n",
            "epoch 251, loss 0.19021812081336975\n",
            "epoch 252, loss 0.18919453024864197\n",
            "epoch 253, loss 0.188152477145195\n",
            "epoch 254, loss 0.18711666762828827\n",
            "epoch 255, loss 0.18612076342105865\n",
            "epoch 256, loss 0.18509960174560547\n",
            "epoch 257, loss 0.18407012522220612\n",
            "epoch 258, loss 0.18307283520698547\n",
            "epoch 259, loss 0.1820610612630844\n",
            "epoch 260, loss 0.18107908964157104\n",
            "epoch 261, loss 0.1800781637430191\n",
            "epoch 262, loss 0.1790856420993805\n",
            "epoch 263, loss 0.1781304031610489\n",
            "epoch 264, loss 0.17715513706207275\n",
            "epoch 265, loss 0.17620499432086945\n",
            "epoch 266, loss 0.17523565888404846\n",
            "epoch 267, loss 0.1742619425058365\n",
            "epoch 268, loss 0.173319473862648\n",
            "epoch 269, loss 0.17237991094589233\n",
            "epoch 270, loss 0.1714704930782318\n",
            "epoch 271, loss 0.17053760588169098\n",
            "epoch 272, loss 0.1695980578660965\n",
            "epoch 273, loss 0.16868597269058228\n",
            "epoch 274, loss 0.1677936166524887\n",
            "epoch 275, loss 0.16687826812267303\n",
            "epoch 276, loss 0.16595400869846344\n",
            "epoch 277, loss 0.16504652798175812\n",
            "epoch 278, loss 0.1641746163368225\n",
            "epoch 279, loss 0.16331622004508972\n",
            "epoch 280, loss 0.1624346673488617\n",
            "epoch 281, loss 0.16154302656650543\n",
            "epoch 282, loss 0.16065499186515808\n",
            "epoch 283, loss 0.15979662537574768\n",
            "epoch 284, loss 0.1589575856924057\n",
            "epoch 285, loss 0.15810012817382812\n",
            "epoch 286, loss 0.1572466492652893\n",
            "epoch 287, loss 0.15642529726028442\n",
            "epoch 288, loss 0.15558497607707977\n",
            "epoch 289, loss 0.15474236011505127\n",
            "epoch 290, loss 0.15392521023750305\n",
            "epoch 291, loss 0.1531258076429367\n",
            "epoch 292, loss 0.1523061990737915\n",
            "epoch 293, loss 0.15147852897644043\n",
            "epoch 294, loss 0.15066541731357574\n",
            "epoch 295, loss 0.14988373219966888\n",
            "epoch 296, loss 0.1491132527589798\n",
            "epoch 297, loss 0.14832210540771484\n",
            "epoch 298, loss 0.1475221812725067\n",
            "epoch 299, loss 0.14674782752990723\n",
            "epoch 300, loss 0.14596252143383026\n",
            "epoch 301, loss 0.14517739415168762\n",
            "epoch 302, loss 0.14441758394241333\n",
            "epoch 303, loss 0.14368577301502228\n",
            "epoch 304, loss 0.1429397165775299\n",
            "epoch 305, loss 0.14218004047870636\n",
            "epoch 306, loss 0.1414203941822052\n",
            "epoch 307, loss 0.1406848132610321\n",
            "epoch 308, loss 0.13996422290802002\n",
            "epoch 309, loss 0.1392281949520111\n",
            "epoch 310, loss 0.13848662376403809\n",
            "epoch 311, loss 0.13776706159114838\n",
            "epoch 312, loss 0.13705037534236908\n",
            "epoch 313, loss 0.13635538518428802\n",
            "epoch 314, loss 0.13564421236515045\n",
            "epoch 315, loss 0.13494844734668732\n",
            "epoch 316, loss 0.13424032926559448\n",
            "epoch 317, loss 0.13353098928928375\n",
            "epoch 318, loss 0.13284292817115784\n",
            "epoch 319, loss 0.1321684867143631\n",
            "epoch 320, loss 0.13147978484630585\n",
            "epoch 321, loss 0.13078661262989044\n",
            "epoch 322, loss 0.13011400401592255\n",
            "epoch 323, loss 0.12946495413780212\n",
            "epoch 324, loss 0.1288042813539505\n",
            "epoch 325, loss 0.12813358008861542\n",
            "epoch 326, loss 0.12748102843761444\n",
            "epoch 327, loss 0.12682051956653595\n",
            "epoch 328, loss 0.1261766403913498\n",
            "epoch 329, loss 0.12552425265312195\n",
            "epoch 330, loss 0.12488819658756256\n",
            "epoch 331, loss 0.12424290180206299\n",
            "epoch 332, loss 0.12361432611942291\n",
            "epoch 333, loss 0.12297625094652176\n",
            "epoch 334, loss 0.12234723567962646\n",
            "epoch 335, loss 0.12173981964588165\n",
            "epoch 336, loss 0.12113875150680542\n",
            "epoch 337, loss 0.12052299082279205\n",
            "epoch 338, loss 0.11990214884281158\n",
            "epoch 339, loss 0.11929843574762344\n",
            "epoch 340, loss 0.11870638281106949\n",
            "epoch 341, loss 0.11810187995433807\n",
            "epoch 342, loss 0.11749330908060074\n",
            "epoch 343, loss 0.11688834428787231\n",
            "epoch 344, loss 0.11630317568778992\n",
            "epoch 345, loss 0.11573060601949692\n",
            "epoch 346, loss 0.11514819413423538\n",
            "epoch 347, loss 0.11457102745771408\n",
            "epoch 348, loss 0.1140131726861\n",
            "epoch 349, loss 0.1134435385465622\n",
            "epoch 350, loss 0.11288636922836304\n",
            "epoch 351, loss 0.11231972277164459\n",
            "epoch 352, loss 0.11175096035003662\n",
            "epoch 353, loss 0.11119911074638367\n",
            "epoch 354, loss 0.11064206063747406\n",
            "epoch 355, loss 0.11009974032640457\n",
            "epoch 356, loss 0.10955001413822174\n",
            "epoch 357, loss 0.10899878293275833\n",
            "epoch 358, loss 0.10846438258886337\n",
            "epoch 359, loss 0.10792477428913116\n",
            "epoch 360, loss 0.10738503187894821\n",
            "epoch 361, loss 0.10686956346035004\n",
            "epoch 362, loss 0.10635142028331757\n",
            "epoch 363, loss 0.10582786798477173\n",
            "epoch 364, loss 0.10531704127788544\n",
            "epoch 365, loss 0.10479985922574997\n",
            "epoch 366, loss 0.10428124666213989\n",
            "epoch 367, loss 0.10377747565507889\n",
            "epoch 368, loss 0.10326869040727615\n",
            "epoch 369, loss 0.10275892913341522\n",
            "epoch 370, loss 0.10226471722126007\n",
            "epoch 371, loss 0.10176592320203781\n",
            "epoch 372, loss 0.10126607120037079\n",
            "epoch 373, loss 0.10076859593391418\n",
            "epoch 374, loss 0.10028667747974396\n",
            "epoch 375, loss 0.09980107098817825\n",
            "epoch 376, loss 0.09932217746973038\n",
            "epoch 377, loss 0.09885958582162857\n",
            "epoch 378, loss 0.09838711470365524\n",
            "epoch 379, loss 0.09791085124015808\n",
            "epoch 380, loss 0.0974348783493042\n",
            "epoch 381, loss 0.09697357565164566\n",
            "epoch 382, loss 0.09650801867246628\n",
            "epoch 383, loss 0.09604158252477646\n",
            "epoch 384, loss 0.09557665139436722\n",
            "epoch 385, loss 0.0951266810297966\n",
            "epoch 386, loss 0.09467270225286484\n",
            "epoch 387, loss 0.09421789646148682\n",
            "epoch 388, loss 0.09376445412635803\n",
            "epoch 389, loss 0.09331387281417847\n",
            "epoch 390, loss 0.09287825226783752\n",
            "epoch 391, loss 0.09244614839553833\n",
            "epoch 392, loss 0.09201481193304062\n",
            "epoch 393, loss 0.09158021956682205\n",
            "epoch 394, loss 0.09114574640989304\n",
            "epoch 395, loss 0.09071322530508041\n",
            "epoch 396, loss 0.09028361737728119\n",
            "epoch 397, loss 0.08986759185791016\n",
            "epoch 398, loss 0.08944792300462723\n",
            "epoch 399, loss 0.08902687579393387\n",
            "epoch 400, loss 0.08860651403665543\n",
            "epoch 401, loss 0.08818812668323517\n",
            "epoch 402, loss 0.08777251839637756\n",
            "epoch 403, loss 0.08735993504524231\n",
            "epoch 404, loss 0.08695059269666672\n",
            "epoch 405, loss 0.08654431253671646\n",
            "epoch 406, loss 0.08614099770784378\n",
            "epoch 407, loss 0.08574686199426651\n",
            "epoch 408, loss 0.08535692095756531\n",
            "epoch 409, loss 0.08496536314487457\n",
            "epoch 410, loss 0.08457405865192413\n",
            "epoch 411, loss 0.08418384194374084\n",
            "epoch 412, loss 0.08379533141851425\n",
            "epoch 413, loss 0.08340877294540405\n",
            "epoch 414, loss 0.08302425593137741\n",
            "epoch 415, loss 0.08264195173978806\n",
            "epoch 416, loss 0.08226186782121658\n",
            "epoch 417, loss 0.08188403397798538\n",
            "epoch 418, loss 0.08150840550661087\n",
            "epoch 419, loss 0.08113503456115723\n",
            "epoch 420, loss 0.08076387643814087\n",
            "epoch 421, loss 0.08040095865726471\n",
            "epoch 422, loss 0.08004172146320343\n",
            "epoch 423, loss 0.07968087494373322\n",
            "epoch 424, loss 0.07932012528181076\n",
            "epoch 425, loss 0.07896046340465546\n",
            "epoch 426, loss 0.07860229164361954\n",
            "epoch 427, loss 0.07824604213237762\n",
            "epoch 428, loss 0.07789172232151031\n",
            "epoch 429, loss 0.0775393694639206\n",
            "epoch 430, loss 0.07718901336193085\n",
            "epoch 431, loss 0.07684070616960526\n",
            "epoch 432, loss 0.07649433612823486\n",
            "epoch 433, loss 0.07615000009536743\n",
            "epoch 434, loss 0.07581336796283722\n",
            "epoch 435, loss 0.0754801481962204\n",
            "epoch 436, loss 0.07514526695013046\n",
            "epoch 437, loss 0.07481051236391068\n",
            "epoch 438, loss 0.07447666674852371\n",
            "epoch 439, loss 0.07414424419403076\n",
            "epoch 440, loss 0.07381344586610794\n",
            "epoch 441, loss 0.07348441332578659\n",
            "epoch 442, loss 0.07315719127655029\n",
            "epoch 443, loss 0.07283182442188263\n",
            "epoch 444, loss 0.0725083202123642\n",
            "epoch 445, loss 0.07218670099973679\n",
            "epoch 446, loss 0.07186689972877502\n",
            "epoch 447, loss 0.07155433297157288\n",
            "epoch 448, loss 0.07124491035938263\n",
            "epoch 449, loss 0.07093395292758942\n",
            "epoch 450, loss 0.07062296569347382\n",
            "epoch 451, loss 0.07031285017728806\n",
            "epoch 452, loss 0.07000395655632019\n",
            "epoch 453, loss 0.06969656050205231\n",
            "epoch 454, loss 0.06939077377319336\n",
            "epoch 455, loss 0.06908668577671051\n",
            "epoch 456, loss 0.06878424435853958\n",
            "epoch 457, loss 0.06848355382680893\n",
            "epoch 458, loss 0.06818963587284088\n",
            "epoch 459, loss 0.067898690700531\n",
            "epoch 460, loss 0.06760624051094055\n",
            "epoch 461, loss 0.06731374561786652\n",
            "epoch 462, loss 0.06702201813459396\n",
            "epoch 463, loss 0.06673142313957214\n",
            "epoch 464, loss 0.06644222885370255\n",
            "epoch 465, loss 0.06615452468395233\n",
            "epoch 466, loss 0.06586837768554688\n",
            "epoch 467, loss 0.06558380275964737\n",
            "epoch 468, loss 0.06530572474002838\n",
            "epoch 469, loss 0.06503041088581085\n",
            "epoch 470, loss 0.06475361436605453\n",
            "epoch 471, loss 0.06447675079107285\n",
            "epoch 472, loss 0.06420058012008667\n",
            "epoch 473, loss 0.06392549723386765\n",
            "epoch 474, loss 0.0636516660451889\n",
            "epoch 475, loss 0.06337928771972656\n",
            "epoch 476, loss 0.06310833245515823\n",
            "epoch 477, loss 0.06283888220787048\n",
            "epoch 478, loss 0.06257558614015579\n",
            "epoch 479, loss 0.06231490150094032\n",
            "epoch 480, loss 0.062052786350250244\n",
            "epoch 481, loss 0.06179056689143181\n",
            "epoch 482, loss 0.06152899190783501\n",
            "epoch 483, loss 0.061268407851457596\n",
            "epoch 484, loss 0.06100902333855629\n",
            "epoch 485, loss 0.06075097247958183\n",
            "epoch 486, loss 0.06049134582281113\n",
            "epoch 487, loss 0.060234520584344864\n",
            "epoch 488, loss 0.05998251959681511\n",
            "epoch 489, loss 0.05973070114850998\n",
            "epoch 490, loss 0.059477873146533966\n",
            "epoch 491, loss 0.059228383004665375\n",
            "epoch 492, loss 0.058980487287044525\n",
            "epoch 493, loss 0.058732032775878906\n",
            "epoch 494, loss 0.05848410353064537\n",
            "epoch 495, loss 0.05824166163802147\n",
            "epoch 496, loss 0.05800189450383186\n",
            "epoch 497, loss 0.057763710618019104\n",
            "epoch 498, loss 0.057526081800460815\n",
            "epoch 499, loss 0.057287100702524185\n",
            "epoch 500, loss 0.0570480152964592\n",
            "epoch 501, loss 0.056809619069099426\n",
            "epoch 502, loss 0.05657242238521576\n",
            "epoch 503, loss 0.056340839713811874\n",
            "epoch 504, loss 0.05611182749271393\n",
            "epoch 505, loss 0.05588170140981674\n",
            "epoch 506, loss 0.055651724338531494\n",
            "epoch 507, loss 0.055422354489564896\n",
            "epoch 508, loss 0.0551939532160759\n",
            "epoch 509, loss 0.05496668070554733\n",
            "epoch 510, loss 0.05474052205681801\n",
            "epoch 511, loss 0.054519567638635635\n",
            "epoch 512, loss 0.05430077016353607\n",
            "epoch 513, loss 0.054080694913864136\n",
            "epoch 514, loss 0.05386044457554817\n",
            "epoch 515, loss 0.053640637546777725\n",
            "epoch 516, loss 0.05342164263129234\n",
            "epoch 517, loss 0.05320359021425247\n",
            "epoch 518, loss 0.052986644208431244\n",
            "epoch 519, loss 0.052774690091609955\n",
            "epoch 520, loss 0.05256478488445282\n",
            "epoch 521, loss 0.05235368758440018\n",
            "epoch 522, loss 0.05214237794280052\n",
            "epoch 523, loss 0.051931507885456085\n",
            "epoch 524, loss 0.05172141268849373\n",
            "epoch 525, loss 0.051512230187654495\n",
            "epoch 526, loss 0.051307812333106995\n",
            "epoch 527, loss 0.05110554024577141\n",
            "epoch 528, loss 0.050902120769023895\n",
            "epoch 529, loss 0.05069851875305176\n",
            "epoch 530, loss 0.050495270639657974\n",
            "epoch 531, loss 0.05029264837503433\n",
            "epoch 532, loss 0.050090860575437546\n",
            "epoch 533, loss 0.04989367350935936\n",
            "epoch 534, loss 0.04969833418726921\n",
            "epoch 535, loss 0.04950174316763878\n",
            "epoch 536, loss 0.04930499941110611\n",
            "epoch 537, loss 0.049108631908893585\n",
            "epoch 538, loss 0.048912931233644485\n",
            "epoch 539, loss 0.048718057572841644\n",
            "epoch 540, loss 0.04852772131562233\n",
            "epoch 541, loss 0.04833921417593956\n",
            "epoch 542, loss 0.048149511218070984\n",
            "epoch 543, loss 0.04795961081981659\n",
            "epoch 544, loss 0.047770071774721146\n",
            "epoch 545, loss 0.047581180930137634\n",
            "epoch 546, loss 0.04739312082529068\n",
            "epoch 547, loss 0.047209396958351135\n",
            "epoch 548, loss 0.04702741280198097\n",
            "epoch 549, loss 0.04684427008032799\n",
            "epoch 550, loss 0.04666094854474068\n",
            "epoch 551, loss 0.04647796228528023\n",
            "epoch 552, loss 0.04629560187458992\n",
            "epoch 553, loss 0.04611736163496971\n",
            "epoch 554, loss 0.04594080522656441\n",
            "epoch 555, loss 0.04576307535171509\n",
            "epoch 556, loss 0.04558510705828667\n",
            "epoch 557, loss 0.045407477766275406\n",
            "epoch 558, loss 0.045230425894260406\n",
            "epoch 559, loss 0.04505414515733719\n",
            "epoch 560, loss 0.04488193243741989\n",
            "epoch 561, loss 0.04471142590045929\n",
            "epoch 562, loss 0.04453972727060318\n",
            "epoch 563, loss 0.044367820024490356\n",
            "epoch 564, loss 0.04419626668095589\n",
            "epoch 565, loss 0.044025223702192307\n",
            "epoch 566, loss 0.04385807737708092\n",
            "epoch 567, loss 0.04369253292679787\n",
            "epoch 568, loss 0.043525852262973785\n",
            "epoch 569, loss 0.04335890710353851\n",
            "epoch 570, loss 0.04319228231906891\n",
            "epoch 571, loss 0.043026186525821686\n",
            "epoch 572, loss 0.042863879352808\n",
            "epoch 573, loss 0.04270309954881668\n",
            "epoch 574, loss 0.042541228234767914\n",
            "epoch 575, loss 0.04237906262278557\n",
            "epoch 576, loss 0.04221717268228531\n",
            "epoch 577, loss 0.04205891862511635\n",
            "epoch 578, loss 0.04190200939774513\n",
            "epoch 579, loss 0.04174397513270378\n",
            "epoch 580, loss 0.041585665196180344\n",
            "epoch 581, loss 0.04142763465642929\n",
            "epoch 582, loss 0.04127006605267525\n",
            "epoch 583, loss 0.04111612215638161\n",
            "epoch 584, loss 0.04096364974975586\n",
            "epoch 585, loss 0.040810056030750275\n",
            "epoch 586, loss 0.04065623879432678\n",
            "epoch 587, loss 0.04050268605351448\n",
            "epoch 588, loss 0.040352534502744675\n",
            "epoch 589, loss 0.040203679352998734\n",
            "epoch 590, loss 0.040053706616163254\n",
            "epoch 591, loss 0.03990350663661957\n",
            "epoch 592, loss 0.039753518998622894\n",
            "epoch 593, loss 0.039604004472494125\n",
            "epoch 594, loss 0.039457909762859344\n",
            "epoch 595, loss 0.039313215762376785\n",
            "epoch 596, loss 0.039167437702417374\n",
            "epoch 597, loss 0.03902146965265274\n",
            "epoch 598, loss 0.03887847810983658\n",
            "epoch 599, loss 0.03873663768172264\n",
            "epoch 600, loss 0.03859362751245499\n",
            "epoch 601, loss 0.03845034912228584\n",
            "epoch 602, loss 0.03830721974372864\n",
            "epoch 603, loss 0.038164522498846054\n",
            "epoch 604, loss 0.03802517056465149\n",
            "epoch 605, loss 0.037887100130319595\n",
            "epoch 606, loss 0.037747956812381744\n",
            "epoch 607, loss 0.03760863468050957\n",
            "epoch 608, loss 0.03747217357158661\n",
            "epoch 609, loss 0.0373368039727211\n",
            "epoch 610, loss 0.03720032796263695\n",
            "epoch 611, loss 0.03706354275345802\n",
            "epoch 612, loss 0.036926958709955215\n",
            "epoch 613, loss 0.036793384701013565\n",
            "epoch 614, loss 0.03666098043322563\n",
            "epoch 615, loss 0.03652755543589592\n",
            "epoch 616, loss 0.036393921822309494\n",
            "epoch 617, loss 0.03626048192381859\n",
            "epoch 618, loss 0.03613002598285675\n",
            "epoch 619, loss 0.03600071743130684\n",
            "epoch 620, loss 0.03587036579847336\n",
            "epoch 621, loss 0.035739753395318985\n",
            "epoch 622, loss 0.0356118381023407\n",
            "epoch 623, loss 0.035484910011291504\n",
            "epoch 624, loss 0.03535694628953934\n",
            "epoch 625, loss 0.03522862493991852\n",
            "epoch 626, loss 0.03510301560163498\n",
            "epoch 627, loss 0.03497833386063576\n",
            "epoch 628, loss 0.03485259413719177\n",
            "epoch 629, loss 0.034726615995168686\n",
            "epoch 630, loss 0.034600745886564255\n",
            "epoch 631, loss 0.03447769954800606\n",
            "epoch 632, loss 0.03435574471950531\n",
            "epoch 633, loss 0.03423280268907547\n",
            "epoch 634, loss 0.03410964086651802\n",
            "epoch 635, loss 0.03398900479078293\n",
            "epoch 636, loss 0.033869341015815735\n",
            "epoch 637, loss 0.03374865651130676\n",
            "epoch 638, loss 0.03362768515944481\n",
            "epoch 639, loss 0.03350921720266342\n",
            "epoch 640, loss 0.03339167684316635\n",
            "epoch 641, loss 0.03327314183115959\n",
            "epoch 642, loss 0.03315427526831627\n",
            "epoch 643, loss 0.03303791582584381\n",
            "epoch 644, loss 0.03292242810130119\n",
            "epoch 645, loss 0.03280596062541008\n",
            "epoch 646, loss 0.03268924355506897\n",
            "epoch 647, loss 0.032574914395809174\n",
            "epoch 648, loss 0.03246144950389862\n",
            "epoch 649, loss 0.03234705328941345\n",
            "epoch 650, loss 0.03223234787583351\n",
            "epoch 651, loss 0.03212006762623787\n",
            "epoch 652, loss 0.03200864791870117\n",
            "epoch 653, loss 0.03189622983336449\n",
            "epoch 654, loss 0.031783558428287506\n",
            "epoch 655, loss 0.03167322650551796\n",
            "epoch 656, loss 0.03156372532248497\n",
            "epoch 657, loss 0.03145328909158707\n",
            "epoch 658, loss 0.03134477138519287\n",
            "epoch 659, loss 0.031236885115504265\n",
            "epoch 660, loss 0.031127963215112686\n",
            "epoch 661, loss 0.031018702313303947\n",
            "epoch 662, loss 0.030911710113286972\n",
            "epoch 663, loss 0.030805513262748718\n",
            "epoch 664, loss 0.030698370188474655\n",
            "epoch 665, loss 0.030590958893299103\n",
            "epoch 666, loss 0.030485792085528374\n",
            "epoch 667, loss 0.03038143739104271\n",
            "epoch 668, loss 0.030276156961917877\n",
            "epoch 669, loss 0.03017273359000683\n",
            "epoch 670, loss 0.03006988950073719\n",
            "epoch 671, loss 0.029966067522764206\n",
            "epoch 672, loss 0.029861880466341972\n",
            "epoch 673, loss 0.029759852215647697\n",
            "epoch 674, loss 0.02965862676501274\n",
            "epoch 675, loss 0.02955644391477108\n",
            "epoch 676, loss 0.029456086456775665\n",
            "epoch 677, loss 0.029356341809034348\n",
            "epoch 678, loss 0.029255539178848267\n",
            "epoch 679, loss 0.029154470190405846\n",
            "epoch 680, loss 0.02905544452369213\n",
            "epoch 681, loss 0.02895720861852169\n",
            "epoch 682, loss 0.02885805442929268\n",
            "epoch 683, loss 0.02876066416501999\n",
            "epoch 684, loss 0.02866385318338871\n",
            "epoch 685, loss 0.028566066175699234\n",
            "epoch 686, loss 0.028469931334257126\n",
            "epoch 687, loss 0.028374362736940384\n",
            "epoch 688, loss 0.028277775272727013\n",
            "epoch 689, loss 0.028180891647934914\n",
            "epoch 690, loss 0.02808598056435585\n",
            "epoch 691, loss 0.027991842478513718\n",
            "epoch 692, loss 0.027896812185645103\n",
            "epoch 693, loss 0.027803458273410797\n",
            "epoch 694, loss 0.027710655704140663\n",
            "epoch 695, loss 0.027616899460554123\n",
            "epoch 696, loss 0.027524776756763458\n",
            "epoch 697, loss 0.027433142066001892\n",
            "epoch 698, loss 0.027340540662407875\n",
            "epoch 699, loss 0.027249595150351524\n",
            "epoch 700, loss 0.027159156277775764\n",
            "epoch 701, loss 0.027067814022302628\n",
            "epoch 702, loss 0.026978062465786934\n",
            "epoch 703, loss 0.0268887747079134\n",
            "epoch 704, loss 0.026798518374562263\n",
            "epoch 705, loss 0.02670980803668499\n",
            "epoch 706, loss 0.026621537283062935\n",
            "epoch 707, loss 0.026532350108027458\n",
            "epoch 708, loss 0.02644469402730465\n",
            "epoch 709, loss 0.02635752595961094\n",
            "epoch 710, loss 0.02626938745379448\n",
            "epoch 711, loss 0.026182817295193672\n",
            "epoch 712, loss 0.026096709072589874\n",
            "epoch 713, loss 0.0260096937417984\n",
            "epoch 714, loss 0.025924168527126312\n",
            "epoch 715, loss 0.02583913691341877\n",
            "epoch 716, loss 0.02575315348803997\n",
            "epoch 717, loss 0.025668703019618988\n",
            "epoch 718, loss 0.025584710761904716\n",
            "epoch 719, loss 0.025499820709228516\n",
            "epoch 720, loss 0.025416402146220207\n",
            "epoch 721, loss 0.025333452969789505\n",
            "epoch 722, loss 0.025249596685171127\n",
            "epoch 723, loss 0.02516721747815609\n",
            "epoch 724, loss 0.02508527785539627\n",
            "epoch 725, loss 0.02500244230031967\n",
            "epoch 726, loss 0.02492108754813671\n",
            "epoch 727, loss 0.02484012395143509\n",
            "epoch 728, loss 0.024760043248534203\n",
            "epoch 729, loss 0.024680087342858315\n",
            "epoch 730, loss 0.024599092081189156\n",
            "epoch 731, loss 0.024519428610801697\n",
            "epoch 732, loss 0.02444014698266983\n",
            "epoch 733, loss 0.02435997687280178\n",
            "epoch 734, loss 0.024281181395053864\n",
            "epoch 735, loss 0.024202844128012657\n",
            "epoch 736, loss 0.024123601615428925\n",
            "epoch 737, loss 0.02404576912522316\n",
            "epoch 738, loss 0.023968350142240524\n",
            "epoch 739, loss 0.023891745135188103\n",
            "epoch 740, loss 0.023815253749489784\n",
            "epoch 741, loss 0.023737765848636627\n",
            "epoch 742, loss 0.02366156131029129\n",
            "epoch 743, loss 0.023585699498653412\n",
            "epoch 744, loss 0.023509010672569275\n",
            "epoch 745, loss 0.023433631286025047\n",
            "epoch 746, loss 0.023358672857284546\n",
            "epoch 747, loss 0.02328450046479702\n",
            "epoch 748, loss 0.023210441693663597\n",
            "epoch 749, loss 0.023135405033826828\n",
            "epoch 750, loss 0.023061590269207954\n",
            "epoch 751, loss 0.022988131269812584\n",
            "epoch 752, loss 0.022915448993444443\n",
            "epoch 753, loss 0.02284281514585018\n",
            "epoch 754, loss 0.022769227623939514\n",
            "epoch 755, loss 0.02269686944782734\n",
            "epoch 756, loss 0.022624829784035683\n",
            "epoch 757, loss 0.0225520022213459\n",
            "epoch 758, loss 0.022480400279164314\n",
            "epoch 759, loss 0.02241075411438942\n",
            "epoch 760, loss 0.022340718656778336\n",
            "epoch 761, loss 0.022269416600465775\n",
            "epoch 762, loss 0.022197626531124115\n",
            "epoch 763, loss 0.02212720923125744\n",
            "epoch 764, loss 0.022058837115764618\n",
            "epoch 765, loss 0.021990083158016205\n",
            "epoch 766, loss 0.021920155733823776\n",
            "epoch 767, loss 0.02184971049427986\n",
            "epoch 768, loss 0.021780669689178467\n",
            "epoch 769, loss 0.0217136237770319\n",
            "epoch 770, loss 0.021646233275532722\n",
            "epoch 771, loss 0.021577607840299606\n",
            "epoch 772, loss 0.021510034799575806\n",
            "epoch 773, loss 0.021442726254463196\n",
            "epoch 774, loss 0.021374579519033432\n",
            "epoch 775, loss 0.02130761183798313\n",
            "epoch 776, loss 0.021242450922727585\n",
            "epoch 777, loss 0.021176910027861595\n",
            "epoch 778, loss 0.021110177040100098\n",
            "epoch 779, loss 0.021042948588728905\n",
            "epoch 780, loss 0.02097708359360695\n",
            "epoch 781, loss 0.02091304399073124\n",
            "epoch 782, loss 0.02084868960082531\n",
            "epoch 783, loss 0.020783202722668648\n",
            "epoch 784, loss 0.02071869932115078\n",
            "epoch 785, loss 0.020654434338212013\n",
            "epoch 786, loss 0.020590828731656075\n",
            "epoch 787, loss 0.020527228713035583\n",
            "epoch 788, loss 0.020464157685637474\n",
            "epoch 789, loss 0.0204011257737875\n",
            "epoch 790, loss 0.020337160676717758\n",
            "epoch 791, loss 0.020274266600608826\n",
            "epoch 792, loss 0.02021303027868271\n",
            "epoch 793, loss 0.020151397213339806\n",
            "epoch 794, loss 0.020088644698262215\n",
            "epoch 795, loss 0.02002676948904991\n",
            "epoch 796, loss 0.019965145736932755\n",
            "epoch 797, loss 0.019904114305973053\n",
            "epoch 798, loss 0.019843120127916336\n",
            "epoch 799, loss 0.019781267270445824\n",
            "epoch 800, loss 0.019720470532774925\n",
            "epoch 801, loss 0.019661301746964455\n",
            "epoch 802, loss 0.019601736217737198\n",
            "epoch 803, loss 0.019541066139936447\n",
            "epoch 804, loss 0.019481267780065536\n",
            "epoch 805, loss 0.019423045217990875\n",
            "epoch 806, loss 0.01936442404985428\n",
            "epoch 807, loss 0.019304640591144562\n",
            "epoch 808, loss 0.01924576424062252\n",
            "epoch 809, loss 0.01918708346784115\n",
            "epoch 810, loss 0.019128980115056038\n",
            "epoch 811, loss 0.019070886075496674\n",
            "epoch 812, loss 0.019013315439224243\n",
            "epoch 813, loss 0.018955696374177933\n",
            "epoch 814, loss 0.01889858767390251\n",
            "epoch 815, loss 0.018841417506337166\n",
            "epoch 816, loss 0.018783412873744965\n",
            "epoch 817, loss 0.01872636005282402\n",
            "epoch 818, loss 0.018670853227376938\n",
            "epoch 819, loss 0.018615012988448143\n",
            "epoch 820, loss 0.018558137118816376\n",
            "epoch 821, loss 0.018502160906791687\n",
            "epoch 822, loss 0.01844760775566101\n",
            "epoch 823, loss 0.018392598256468773\n",
            "epoch 824, loss 0.01833656057715416\n",
            "epoch 825, loss 0.018281320109963417\n",
            "epoch 826, loss 0.01822749339044094\n",
            "epoch 827, loss 0.018173281103372574\n",
            "epoch 828, loss 0.018117986619472504\n",
            "epoch 829, loss 0.0180634967982769\n",
            "epoch 830, loss 0.018010450527071953\n",
            "epoch 831, loss 0.017957009375095367\n",
            "epoch 832, loss 0.017902521416544914\n",
            "epoch 833, loss 0.01784881390631199\n",
            "epoch 834, loss 0.017796551808714867\n",
            "epoch 835, loss 0.017743859440088272\n",
            "epoch 836, loss 0.01769019290804863\n",
            "epoch 837, loss 0.01763729937374592\n",
            "epoch 838, loss 0.01758582703769207\n",
            "epoch 839, loss 0.017533909529447556\n",
            "epoch 840, loss 0.017482241615653038\n",
            "epoch 841, loss 0.01743042655289173\n",
            "epoch 842, loss 0.017378918826580048\n",
            "epoch 843, loss 0.017327412962913513\n",
            "epoch 844, loss 0.017276281490921974\n",
            "epoch 845, loss 0.017225101590156555\n",
            "epoch 846, loss 0.017174312844872475\n",
            "epoch 847, loss 0.017123514786362648\n",
            "epoch 848, loss 0.01707308553159237\n",
            "epoch 849, loss 0.017023799940943718\n",
            "epoch 850, loss 0.016974061727523804\n",
            "epoch 851, loss 0.016923265531659126\n",
            "epoch 852, loss 0.01687319576740265\n",
            "epoch 853, loss 0.016824446618556976\n",
            "epoch 854, loss 0.016775300726294518\n",
            "epoch 855, loss 0.01672520861029625\n",
            "epoch 856, loss 0.016675852239131927\n",
            "epoch 857, loss 0.01662779413163662\n",
            "epoch 858, loss 0.016579370945692062\n",
            "epoch 859, loss 0.016531135886907578\n",
            "epoch 860, loss 0.016482757404446602\n",
            "epoch 861, loss 0.01643470674753189\n",
            "epoch 862, loss 0.016387710347771645\n",
            "epoch 863, loss 0.016340242698788643\n",
            "epoch 864, loss 0.01629180833697319\n",
            "epoch 865, loss 0.016244027763605118\n",
            "epoch 866, loss 0.016197524964809418\n",
            "epoch 867, loss 0.01615062542259693\n",
            "epoch 868, loss 0.016103895381093025\n",
            "epoch 869, loss 0.016057034954428673\n",
            "epoch 870, loss 0.016010524705052376\n",
            "epoch 871, loss 0.015963951125741005\n",
            "epoch 872, loss 0.015917707234621048\n",
            "epoch 873, loss 0.015872521325945854\n",
            "epoch 874, loss 0.01582690142095089\n",
            "epoch 875, loss 0.01578027382493019\n",
            "epoch 876, loss 0.01573435217142105\n",
            "epoch 877, loss 0.01568959653377533\n",
            "epoch 878, loss 0.015645597130060196\n",
            "epoch 879, loss 0.015600958839058876\n",
            "epoch 880, loss 0.015555271878838539\n",
            "epoch 881, loss 0.015510174445807934\n",
            "epoch 882, loss 0.015466245822608471\n",
            "epoch 883, loss 0.015421967022120953\n",
            "epoch 884, loss 0.015377814881503582\n",
            "epoch 885, loss 0.015334626659750938\n",
            "epoch 886, loss 0.015290903858840466\n",
            "epoch 887, loss 0.015246240422129631\n",
            "epoch 888, loss 0.015202218666672707\n",
            "epoch 889, loss 0.015159337781369686\n",
            "epoch 890, loss 0.015117108821868896\n",
            "epoch 891, loss 0.015074312686920166\n",
            "epoch 892, loss 0.015030489303171635\n",
            "epoch 893, loss 0.014987253583967686\n",
            "epoch 894, loss 0.01494515035301447\n",
            "epoch 895, loss 0.014903686009347439\n",
            "epoch 896, loss 0.014861627481877804\n",
            "epoch 897, loss 0.014818549156188965\n",
            "epoch 898, loss 0.01477606687694788\n",
            "epoch 899, loss 0.01473466120660305\n",
            "epoch 900, loss 0.014693929813802242\n",
            "epoch 901, loss 0.014652571640908718\n",
            "epoch 902, loss 0.014610261656343937\n",
            "epoch 903, loss 0.014568482525646687\n",
            "epoch 904, loss 0.014527824707329273\n",
            "epoch 905, loss 0.014487792737782001\n",
            "epoch 906, loss 0.014447151683270931\n",
            "epoch 907, loss 0.014405529946088791\n",
            "epoch 908, loss 0.014364467933773994\n",
            "epoch 909, loss 0.0143244918435812\n",
            "epoch 910, loss 0.014285149052739143\n",
            "epoch 911, loss 0.014245206490159035\n",
            "epoch 912, loss 0.014204300940036774\n",
            "epoch 913, loss 0.014163965359330177\n",
            "epoch 914, loss 0.014124678447842598\n",
            "epoch 915, loss 0.014086010865867138\n",
            "epoch 916, loss 0.014046772383153439\n",
            "epoch 917, loss 0.01400752179324627\n",
            "epoch 918, loss 0.01396908424794674\n",
            "epoch 919, loss 0.013930102810263634\n",
            "epoch 920, loss 0.01389025617390871\n",
            "epoch 921, loss 0.01385097112506628\n",
            "epoch 922, loss 0.013812707737088203\n",
            "epoch 923, loss 0.013775056228041649\n",
            "epoch 924, loss 0.013737798668444157\n",
            "epoch 925, loss 0.013699828647077084\n",
            "epoch 926, loss 0.013660919852554798\n",
            "epoch 927, loss 0.013622461818158627\n",
            "epoch 928, loss 0.013585000298917294\n",
            "epoch 929, loss 0.01354811992496252\n",
            "epoch 930, loss 0.013510686345398426\n",
            "epoch 931, loss 0.013473271392285824\n",
            "epoch 932, loss 0.013436583802103996\n",
            "epoch 933, loss 0.013399431481957436\n",
            "epoch 934, loss 0.013362306170165539\n",
            "epoch 935, loss 0.013325045816600323\n",
            "epoch 936, loss 0.013288035988807678\n",
            "epoch 937, loss 0.01325183641165495\n",
            "epoch 938, loss 0.013216137886047363\n",
            "epoch 939, loss 0.013179867528378963\n",
            "epoch 940, loss 0.013143564574420452\n",
            "epoch 941, loss 0.013107109814882278\n",
            "epoch 942, loss 0.013070785440504551\n",
            "epoch 943, loss 0.013035362586379051\n",
            "epoch 944, loss 0.013000346720218658\n",
            "epoch 945, loss 0.012964797206223011\n",
            "epoch 946, loss 0.012929214164614677\n",
            "epoch 947, loss 0.01289349514991045\n",
            "epoch 948, loss 0.012857933528721333\n",
            "epoch 949, loss 0.012823178432881832\n",
            "epoch 950, loss 0.01278888713568449\n",
            "epoch 951, loss 0.012754013761878014\n",
            "epoch 952, loss 0.01271914504468441\n",
            "epoch 953, loss 0.012684956192970276\n",
            "epoch 954, loss 0.012650315649807453\n",
            "epoch 955, loss 0.01261572353541851\n",
            "epoch 956, loss 0.01258182618767023\n",
            "epoch 957, loss 0.01254750695079565\n",
            "epoch 958, loss 0.012513271532952785\n",
            "epoch 959, loss 0.012479716911911964\n",
            "epoch 960, loss 0.012446573935449123\n",
            "epoch 961, loss 0.01241286750882864\n",
            "epoch 962, loss 0.012379133142530918\n",
            "epoch 963, loss 0.012346065603196621\n",
            "epoch 964, loss 0.012312524951994419\n",
            "epoch 965, loss 0.012279060669243336\n",
            "epoch 966, loss 0.012246301397681236\n",
            "epoch 967, loss 0.012213079258799553\n",
            "epoch 968, loss 0.01217989344149828\n",
            "epoch 969, loss 0.012147440575063229\n",
            "epoch 970, loss 0.012115389108657837\n",
            "epoch 971, loss 0.012082760222256184\n",
            "epoch 972, loss 0.012050106190145016\n",
            "epoch 973, loss 0.012018118984997272\n",
            "epoch 974, loss 0.011985663324594498\n",
            "epoch 975, loss 0.01195329986512661\n",
            "epoch 976, loss 0.011921531520783901\n",
            "epoch 977, loss 0.01189021673053503\n",
            "epoch 978, loss 0.011858340352773666\n",
            "epoch 979, loss 0.01182642299681902\n",
            "epoch 980, loss 0.011795149184763432\n",
            "epoch 981, loss 0.011764230206608772\n",
            "epoch 982, loss 0.011732750572264194\n",
            "epoch 983, loss 0.011701241135597229\n",
            "epoch 984, loss 0.011670384556055069\n",
            "epoch 985, loss 0.011639059521257877\n",
            "epoch 986, loss 0.011607813648879528\n",
            "epoch 987, loss 0.011577182449400425\n",
            "epoch 988, loss 0.011546970345079899\n",
            "epoch 989, loss 0.011516948230564594\n",
            "epoch 990, loss 0.011486352421343327\n",
            "epoch 991, loss 0.011455689556896687\n",
            "epoch 992, loss 0.01142561249434948\n",
            "epoch 993, loss 0.011395090259611607\n",
            "epoch 994, loss 0.011364666745066643\n",
            "epoch 995, loss 0.011334811337292194\n",
            "epoch 996, loss 0.011305343359708786\n",
            "epoch 997, loss 0.01127608586102724\n",
            "epoch 998, loss 0.011246285401284695\n",
            "epoch 999, loss 0.011216375976800919\n"
          ]
        }
      ],
      "source": [
        "n_epochs=1000\n",
        "loss_list=[]\n",
        "\n",
        "#n_epochs\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "      \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x.float())\n",
        "        loss = criterion(y_hat,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.data)\n",
        "        \n",
        "        \n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXpoEgK-RDoR",
        "outputId": "d6920a9c-deb6-4b2c-b0db-11312fb97ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0H_oRJIRTth",
        "outputId": "4448631a-a2dd-4a37-e598-3afef80826e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (fc1): Linear(in_features=4, out_features=5, bias=True)\n",
              "  (ha): ReLU()\n",
              "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "RkGXmudfSpqk"
      },
      "outputs": [],
      "source": [
        "y_hat = model(torch.from_numpy(X_train).float())\n",
        "y_ = torch.argmax(y_hat.data,1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duMlWLSmZYyT",
        "outputId": "68a72ddd-9958-4be6-d356-59ee7e81bafc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[32,  0,  0],\n",
              "       [ 0, 29,  1],\n",
              "       [ 0,  0, 34]])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgcinoT7Zioo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP6c_ptiZie6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxIFfIg755fs"
      },
      "outputs": [],
      "source": [
        "#@title Forma \"elegante\" para mostrar a evolução do treinamento.\n",
        "\n",
        "epochs = 50\n",
        "for e in tqdm(range(1, epochs+1)):\n",
        "  \n",
        "  y_hat = np.array([])\n",
        "\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "  model.train()\n",
        "  for X_train_batch, y_train_batch in train_loader:\n",
        "      X, y = X_train_batch.to(device), y_train_batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      y_pred = model(X)\n",
        "      \n",
        "      loss = criterion(y_pred, y)\n",
        "      acc = accuracy(y_pred, y)\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      train_epoch_loss += loss.item()\n",
        "      train_epoch_acc += acc.item()\n",
        "      y_p = torch.argmax(y_pred, dim=1)\n",
        "      y_hat = np.concatenate((y_hat, y_p))\n",
        "  \n",
        "  _, val_loss, val_acc = evaluate(model, val_loader, criterion, device, binary=binary)\n",
        "\n",
        "  history['acc_train'].append(train_epoch_acc)\n",
        "  history['loss_train'].append(train_epoch_loss)\n",
        "  history['acc_val'].append(val_acc)\n",
        "  history['loss_val'].append(val_loss)\n",
        "  \n",
        "  print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.3f} | Val Loss: {val_loss/len(val_loader):.4f} | Train Acc: {train_epoch_acc/len(train_loader):.4f}| Val Acc: {val_acc/len(val_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ktv5UUX55c_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fyYxJN355aH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3xF0sUO55XN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS_5fiEU55UM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64PhZZLA5suw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import seaborn as sns \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV6PcD2W5suw"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data(path=\"mnist.npz\")\n",
        "\n",
        "flatten_x_train = x_train.reshape(-1,28*28)\n",
        "flatten_x_test = x_test.reshape(-1,28*28)\n",
        "\n",
        "norm_x_train = flatten_x_train/255.0\n",
        "norm_x_test = flatten_x_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNUwZ8Sn5suw"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "encoded_Yt = encoder.transform(y_train)\n",
        "\n",
        "encoded_Ytst = encoder.transform(y_test)\n",
        "\n",
        "encoded_y_train = to_categorical(encoded_Yt)\n",
        "encoded_y_test = to_categorical(encoded_Ytst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXHub_uA5sux"
      },
      "outputs": [],
      "source": [
        "n_input = flatten_x_train.shape[1]\n",
        "n_output = encoded_y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pogJtJR55sux"
      },
      "outputs": [],
      "source": [
        "def create_model(hidden_neurons = [4], hidden_activation = ['relu'], output_activation='softmax',lr=0.05):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_neurons[0], input_dim=n_input, activation='relu'))\n",
        "    for i in range(1,len(hidden_neurons)):\n",
        "        model.add(Dense(hidden_neurons[i], input_dim=hidden_neurons[i-1], activation='relu'))\n",
        "        \n",
        "\n",
        "    model.add(Dense(n_output, activation=output_activation))\n",
        "    # Compile model\n",
        "    opt = Adam(lr=lr)\n",
        "    model.compile(\n",
        "                    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                    metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfx1dHPM5sux"
      },
      "outputs": [],
      "source": [
        "model = create_model(hidden_neurons=[80],lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrxoFPRJ5sux",
        "outputId": "33887cf1-f68b-4fd7-ac17-bb16f70b2b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 80)                62800     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                810       \n",
            "=================================================================\n",
            "Total params: 63,610\n",
            "Trainable params: 63,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef-lfvdD5sux",
        "outputId": "7de4949f-8eef-487e-de4f-4bdca06f1dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "1500/1500 [==============================] - 1s 774us/step - loss: 0.3212 - categorical_accuracy: 0.9110 - val_loss: 0.1830 - val_categorical_accuracy: 0.9463\n",
            "Epoch 2/60\n",
            "1500/1500 [==============================] - 1s 695us/step - loss: 0.1567 - categorical_accuracy: 0.9538 - val_loss: 0.1420 - val_categorical_accuracy: 0.9587\n",
            "Epoch 3/60\n",
            "1500/1500 [==============================] - 1s 674us/step - loss: 0.1109 - categorical_accuracy: 0.9671 - val_loss: 0.1181 - val_categorical_accuracy: 0.9663\n",
            "Epoch 4/60\n",
            "1500/1500 [==============================] - 1s 674us/step - loss: 0.0849 - categorical_accuracy: 0.9753 - val_loss: 0.1099 - val_categorical_accuracy: 0.9673\n",
            "Epoch 5/60\n",
            "1500/1500 [==============================] - 1s 674us/step - loss: 0.0685 - categorical_accuracy: 0.9794 - val_loss: 0.1075 - val_categorical_accuracy: 0.9678\n",
            "Epoch 6/60\n",
            "1500/1500 [==============================] - 1s 669us/step - loss: 0.0555 - categorical_accuracy: 0.9827 - val_loss: 0.0959 - val_categorical_accuracy: 0.9731\n",
            "Epoch 7/60\n",
            "1500/1500 [==============================] - 1s 672us/step - loss: 0.0467 - categorical_accuracy: 0.9866 - val_loss: 0.1036 - val_categorical_accuracy: 0.9701\n",
            "Epoch 8/60\n",
            "1500/1500 [==============================] - 1s 687us/step - loss: 0.0390 - categorical_accuracy: 0.9878 - val_loss: 0.1012 - val_categorical_accuracy: 0.9709\n",
            "Epoch 9/60\n",
            "1500/1500 [==============================] - 1s 697us/step - loss: 0.0329 - categorical_accuracy: 0.9900 - val_loss: 0.0995 - val_categorical_accuracy: 0.9695\n",
            "Epoch 10/60\n",
            "1500/1500 [==============================] - 1s 689us/step - loss: 0.0274 - categorical_accuracy: 0.9921 - val_loss: 0.0961 - val_categorical_accuracy: 0.9728\n",
            "Epoch 11/60\n",
            "1500/1500 [==============================] - 1s 668us/step - loss: 0.0244 - categorical_accuracy: 0.9930 - val_loss: 0.1041 - val_categorical_accuracy: 0.9716\n",
            "Epoch 12/60\n",
            "1500/1500 [==============================] - 1s 677us/step - loss: 0.0207 - categorical_accuracy: 0.9938 - val_loss: 0.1057 - val_categorical_accuracy: 0.9704\n",
            "Epoch 13/60\n",
            "1500/1500 [==============================] - 1s 671us/step - loss: 0.0182 - categorical_accuracy: 0.9947 - val_loss: 0.1055 - val_categorical_accuracy: 0.9721\n",
            "Epoch 14/60\n",
            "1500/1500 [==============================] - 1s 699us/step - loss: 0.0153 - categorical_accuracy: 0.9958 - val_loss: 0.1116 - val_categorical_accuracy: 0.9729\n",
            "Epoch 15/60\n",
            "1500/1500 [==============================] - 1s 677us/step - loss: 0.0132 - categorical_accuracy: 0.9961 - val_loss: 0.1086 - val_categorical_accuracy: 0.9743\n",
            "Epoch 16/60\n",
            "1500/1500 [==============================] - 1s 675us/step - loss: 0.0114 - categorical_accuracy: 0.9967 - val_loss: 0.1130 - val_categorical_accuracy: 0.9716\n",
            "Epoch 17/60\n",
            "1500/1500 [==============================] - 1s 689us/step - loss: 0.0114 - categorical_accuracy: 0.9966 - val_loss: 0.1095 - val_categorical_accuracy: 0.9740\n",
            "Epoch 18/60\n",
            "1500/1500 [==============================] - 1s 676us/step - loss: 0.0097 - categorical_accuracy: 0.9970 - val_loss: 0.1146 - val_categorical_accuracy: 0.9736\n",
            "Epoch 19/60\n",
            "1500/1500 [==============================] - 1s 674us/step - loss: 0.0088 - categorical_accuracy: 0.9975 - val_loss: 0.1067 - val_categorical_accuracy: 0.9747\n",
            "Epoch 20/60\n",
            "1500/1500 [==============================] - 1s 675us/step - loss: 0.0075 - categorical_accuracy: 0.9978 - val_loss: 0.1151 - val_categorical_accuracy: 0.9744\n",
            "Epoch 21/60\n",
            "1500/1500 [==============================] - 1s 681us/step - loss: 0.0072 - categorical_accuracy: 0.9979 - val_loss: 0.1253 - val_categorical_accuracy: 0.9736\n",
            "Epoch 22/60\n",
            "1500/1500 [==============================] - 1s 679us/step - loss: 0.0069 - categorical_accuracy: 0.9980 - val_loss: 0.1198 - val_categorical_accuracy: 0.9758\n",
            "Epoch 23/60\n",
            "1500/1500 [==============================] - 1s 678us/step - loss: 0.0052 - categorical_accuracy: 0.9986 - val_loss: 0.1262 - val_categorical_accuracy: 0.9742\n",
            "Epoch 24/60\n",
            "1500/1500 [==============================] - 1s 675us/step - loss: 0.0061 - categorical_accuracy: 0.9982 - val_loss: 0.1416 - val_categorical_accuracy: 0.9714\n",
            "Epoch 25/60\n",
            "1500/1500 [==============================] - 1s 681us/step - loss: 0.0038 - categorical_accuracy: 0.9990 - val_loss: 0.1456 - val_categorical_accuracy: 0.9723\n",
            "Epoch 26/60\n",
            "1500/1500 [==============================] - 1s 685us/step - loss: 0.0063 - categorical_accuracy: 0.9981 - val_loss: 0.1331 - val_categorical_accuracy: 0.9739\n",
            "Epoch 27/60\n",
            "1500/1500 [==============================] - 1s 679us/step - loss: 0.0057 - categorical_accuracy: 0.9983 - val_loss: 0.1440 - val_categorical_accuracy: 0.9729\n",
            "Epoch 28/60\n",
            "1500/1500 [==============================] - 1s 669us/step - loss: 0.0030 - categorical_accuracy: 0.9993 - val_loss: 0.1581 - val_categorical_accuracy: 0.9722\n",
            "Epoch 29/60\n",
            "1500/1500 [==============================] - 1s 675us/step - loss: 0.0052 - categorical_accuracy: 0.9985 - val_loss: 0.1458 - val_categorical_accuracy: 0.9728\n",
            "Epoch 30/60\n",
            "1500/1500 [==============================] - 1s 669us/step - loss: 0.0051 - categorical_accuracy: 0.9985 - val_loss: 0.1548 - val_categorical_accuracy: 0.9722\n",
            "Epoch 31/60\n",
            "1500/1500 [==============================] - 1s 670us/step - loss: 0.0051 - categorical_accuracy: 0.9984 - val_loss: 0.1776 - val_categorical_accuracy: 0.9707\n",
            "Epoch 32/60\n",
            "1500/1500 [==============================] - 1s 689us/step - loss: 0.0020 - categorical_accuracy: 0.9996 - val_loss: 0.1405 - val_categorical_accuracy: 0.9757\n",
            "Epoch 33/60\n",
            "1500/1500 [==============================] - 1s 673us/step - loss: 0.0014 - categorical_accuracy: 0.9997 - val_loss: 0.2092 - val_categorical_accuracy: 0.9682\n",
            "Epoch 34/60\n",
            "1500/1500 [==============================] - 1s 670us/step - loss: 0.0071 - categorical_accuracy: 0.9977 - val_loss: 0.1501 - val_categorical_accuracy: 0.9731\n",
            "Epoch 35/60\n",
            "1500/1500 [==============================] - 1s 669us/step - loss: 0.0034 - categorical_accuracy: 0.9990 - val_loss: 0.1575 - val_categorical_accuracy: 0.9730\n",
            "Epoch 36/60\n",
            "1500/1500 [==============================] - 1s 671us/step - loss: 0.0032 - categorical_accuracy: 0.9990 - val_loss: 0.1832 - val_categorical_accuracy: 0.9703\n",
            "Epoch 37/60\n",
            "1500/1500 [==============================] - 1s 675us/step - loss: 0.0043 - categorical_accuracy: 0.9987 - val_loss: 0.1704 - val_categorical_accuracy: 0.9722\n",
            "Epoch 38/60\n",
            "1500/1500 [==============================] - 1s 668us/step - loss: 0.0028 - categorical_accuracy: 0.9991 - val_loss: 0.1537 - val_categorical_accuracy: 0.9747\n",
            "Epoch 39/60\n",
            "1500/1500 [==============================] - 1s 679us/step - loss: 0.0028 - categorical_accuracy: 0.9992 - val_loss: 0.1779 - val_categorical_accuracy: 0.9701\n",
            "Epoch 40/60\n",
            "1500/1500 [==============================] - 1s 677us/step - loss: 0.0052 - categorical_accuracy: 0.9980 - val_loss: 0.1742 - val_categorical_accuracy: 0.9743\n",
            "Epoch 41/60\n",
            "1500/1500 [==============================] - 1s 685us/step - loss: 0.0026 - categorical_accuracy: 0.9993 - val_loss: 0.1848 - val_categorical_accuracy: 0.9722\n",
            "Epoch 42/60\n",
            "1500/1500 [==============================] - 1s 694us/step - loss: 0.0040 - categorical_accuracy: 0.9989 - val_loss: 0.1745 - val_categorical_accuracy: 0.9730\n",
            "Epoch 43/60\n",
            "1500/1500 [==============================] - 1s 679us/step - loss: 0.0015 - categorical_accuracy: 0.9996 - val_loss: 0.1626 - val_categorical_accuracy: 0.9743\n",
            "Epoch 44/60\n",
            "1500/1500 [==============================] - 1s 707us/step - loss: 0.0031 - categorical_accuracy: 0.9990 - val_loss: 0.1822 - val_categorical_accuracy: 0.9719\n",
            "Epoch 45/60\n",
            "1500/1500 [==============================] - 1s 702us/step - loss: 0.0027 - categorical_accuracy: 0.9990 - val_loss: 0.1968 - val_categorical_accuracy: 0.9718\n",
            "Epoch 46/60\n",
            "1500/1500 [==============================] - 1s 705us/step - loss: 0.0055 - categorical_accuracy: 0.9982 - val_loss: 0.1844 - val_categorical_accuracy: 0.9741\n",
            "Epoch 47/60\n",
            "1500/1500 [==============================] - 1s 673us/step - loss: 0.0025 - categorical_accuracy: 0.9991 - val_loss: 0.1690 - val_categorical_accuracy: 0.9753\n",
            "Epoch 48/60\n",
            "1500/1500 [==============================] - 1s 674us/step - loss: 0.0024 - categorical_accuracy: 0.9993 - val_loss: 0.1775 - val_categorical_accuracy: 0.9733\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/60\n",
            "1500/1500 [==============================] - 1s 676us/step - loss: 0.0025 - categorical_accuracy: 0.9993 - val_loss: 0.1747 - val_categorical_accuracy: 0.9749\n",
            "Epoch 50/60\n",
            "1500/1500 [==============================] - 1s 677us/step - loss: 0.0024 - categorical_accuracy: 0.9993 - val_loss: 0.1810 - val_categorical_accuracy: 0.9748\n",
            "Epoch 51/60\n",
            "1500/1500 [==============================] - 1s 673us/step - loss: 0.0018 - categorical_accuracy: 0.9995 - val_loss: 0.1995 - val_categorical_accuracy: 0.9725\n",
            "Epoch 52/60\n",
            "1500/1500 [==============================] - 1s 674us/step - loss: 0.0036 - categorical_accuracy: 0.9987 - val_loss: 0.2117 - val_categorical_accuracy: 0.9709\n",
            "Epoch 53/60\n",
            "1500/1500 [==============================] - 1s 679us/step - loss: 0.0020 - categorical_accuracy: 0.9994 - val_loss: 0.1806 - val_categorical_accuracy: 0.9747\n",
            "Epoch 54/60\n",
            "1500/1500 [==============================] - 1s 669us/step - loss: 1.2107e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1712 - val_categorical_accuracy: 0.9759\n",
            "Epoch 55/60\n",
            "1500/1500 [==============================] - 1s 673us/step - loss: 5.1913e-05 - categorical_accuracy: 1.0000 - val_loss: 0.1730 - val_categorical_accuracy: 0.9763\n",
            "Epoch 56/60\n",
            "1500/1500 [==============================] - 1s 675us/step - loss: 4.0502e-05 - categorical_accuracy: 1.0000 - val_loss: 0.1755 - val_categorical_accuracy: 0.9764\n",
            "Epoch 57/60\n",
            "1500/1500 [==============================] - 1s 664us/step - loss: 0.0080 - categorical_accuracy: 0.9979 - val_loss: 0.1846 - val_categorical_accuracy: 0.9737\n",
            "Epoch 58/60\n",
            "1500/1500 [==============================] - 1s 667us/step - loss: 0.0026 - categorical_accuracy: 0.9991 - val_loss: 0.1886 - val_categorical_accuracy: 0.9739\n",
            "Epoch 59/60\n",
            "1500/1500 [==============================] - 1s 670us/step - loss: 0.0018 - categorical_accuracy: 0.9993 - val_loss: 0.1967 - val_categorical_accuracy: 0.9733\n",
            "Epoch 60/60\n",
            "1500/1500 [==============================] - 1s 667us/step - loss: 7.0714e-04 - categorical_accuracy: 0.9999 - val_loss: 0.1933 - val_categorical_accuracy: 0.9739\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1e81f67c9a0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x=norm_x_train,y=encoded_y_train,epochs=60,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48pzLH-E5sux",
        "outputId": "fa9fb892-8939-45d4-f542-8344bb5dcb70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[ 964    0    5    1    0    0    5    1    4    0]\n",
            " [   0 1122    4    2    0    0    3    1    3    0]\n",
            " [   3    1 1004    4    2    0    3    3   12    0]\n",
            " [   0    0    4  995    0    3    0    2    2    4]\n",
            " [   3    0    4    1  948    0    5    5    1   15]\n",
            " [   2    1    1   15    2  859    6    0    4    2]\n",
            " [   4    4    3    1    3    3  940    0    0    0]\n",
            " [   1    4   11    5    1    0    0  985   13    8]\n",
            " [   1    0    7    9    5    1    2    2  943    4]\n",
            " [   4    4    0   10    4    3    0    2    2  980]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.96      0.97      0.97      1032\n",
            "           3       0.95      0.99      0.97      1010\n",
            "           4       0.98      0.97      0.97       982\n",
            "           5       0.99      0.96      0.98       892\n",
            "           6       0.98      0.98      0.98       958\n",
            "           7       0.98      0.96      0.97      1028\n",
            "           8       0.96      0.97      0.96       974\n",
            "           9       0.97      0.97      0.97      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Y_pred = model.predict(norm_x_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Classification Report')\n",
        "target_names = ['0','1','2','3','4','5','6','7','8','9']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cWDjhVN5suy",
        "outputId": "56534261-1066-4acc-8be5-d5a87dc4e04f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 123.0, 'Predicted')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2160x1152 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(30,16))\n",
        "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cmn,annot=True)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--F1im4a5suy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "RN_ex1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d044c6e0ce06759120adefce77398d00e1228e64d92f3f185fbd024a7701b8e0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
